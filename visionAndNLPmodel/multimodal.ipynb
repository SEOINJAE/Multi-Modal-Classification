{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ea0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import time\n",
    "import copy\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, \\\n",
    "                            roc_auc_score, confusion_matrix, classification_report, \\\n",
    "                            matthews_corrcoef, cohen_kappa_score, log_loss\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fd0c2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완벽한 실험 재현성을 위한 랜덤제어\n",
    "random_seed = 28\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37b2d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022e35de",
   "metadata": {},
   "source": [
    "# NLP and Vision Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687f890f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_train_path = \"C:/Users/ANDlab3/Desktop/multimodal/visionAndNLPmodel/data/train.csv\"\n",
    "nlp_val_path =  \"C:/Users/ANDlab3/Desktop/multimodal/visionAndNLPmodel/data/val.csv\"\n",
    "nlp_test_path =  \"C:/Users/ANDlab3/Desktop/multimodal/visionAndNLPmodel/data/test.csv\"\n",
    "\n",
    "# nlp_train_path = \"C:/Users/ANDlab3/Desktop/VisionAndNLP/data/train.csv\"\n",
    "# nlp_val_path = \"C:/Users/ANDlab3/Desktop/VisionAndNLP/data/val.csv\"\n",
    "# nlp_test_path = \"C:/Users/ANDlab3/Desktop/VisionAndNLP/data/test.csv\"\n",
    "\n",
    "nlp_train_data = pd.read_csv(nlp_train_path)\n",
    "nlp_val_data = pd.read_csv(nlp_val_path)\n",
    "nlp_test_data = pd.read_csv(nlp_test_path)\n",
    "\n",
    "X_train = nlp_train_data['productDisplayName']\n",
    "y_train = nlp_train_data['label']\n",
    "\n",
    "X_val = nlp_val_data['productDisplayName']\n",
    "y_val = nlp_val_data['label']\n",
    "\n",
    "X_test = nlp_test_data['productDisplayName']\n",
    "y_test = nlp_test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22a59b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    - Remove entity mentions (eg. '@united')\n",
    "    - Correct errors (eg. '&amp;' to '&')\n",
    "    @param    text (str): a string to be processed.\n",
    "    @return   text (Str): the processed string.\n",
    "    \"\"\"\n",
    "    # Remove '@name'\n",
    "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "\n",
    "    # Replace '&amp;' with '&'\n",
    "    text = re.sub(r'&amp;', '&', text)\n",
    "\n",
    "    # Remove trailing whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6b03c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5a786e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ANDlab3\\anaconda3\\envs\\GAN\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 30\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
    "test_inputs, test_masks = preprocessing_for_bert(X_test)\n",
    "\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "test_labels = torch.tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a3301db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        #transforms.RandomResizedCrop(224),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),    \n",
    "}\n",
    "\n",
    "image_path = \"C:/Users/ANDlab3/Desktop/paper/fashion-dataset/data/\"\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(image_path, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val','test']}\n",
    "\n",
    "\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                             shuffle=False, num_workers=4)\n",
    "              for x in ['train', 'val','test']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "class_num = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c74ef058",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_img_temp = []\n",
    "val_img_label_temp = []\n",
    "for i, j in image_datasets['val']:\n",
    "    val_img_temp.append(i)\n",
    "    val_img_label_temp.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87f0358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img_temp = []\n",
    "test_img_label_temp = []\n",
    "for i, j in image_datasets['test']:\n",
    "    test_img_temp.append(i)\n",
    "    test_img_label_temp.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10fc5281",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_temp = []\n",
    "train_img_label_temp = []\n",
    "for i, j in image_datasets['train']:\n",
    "    train_img_temp.append(i)\n",
    "    train_img_label_temp.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8391ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = torch.stack(train_img_temp, 0)\n",
    "train_img_label = torch.tensor(train_img_label_temp)\n",
    "\n",
    "val_img = torch.stack(val_img_temp, 0)\n",
    "val_img_label = torch.tensor(val_img_label_temp)\n",
    "\n",
    "test_img = torch.stack(test_img_temp, 0)\n",
    "test_img_label = torch.tensor(test_img_label_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22f0ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_img, train_img_label)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels, val_img, val_img_label)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels, test_img, test_img_label)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc99d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6475\n",
      "925\n",
      "1850\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4879de",
   "metadata": {},
   "source": [
    "# Vision Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48817e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "#Changing the number of outputs in the last layer to the number of different item types\n",
    "model_ft.fc = nn.Linear(num_ftrs, 500)\n",
    "pre_model= model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f98daf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet_classifier(nn.Module):\n",
    "    def __init__(self, pre_model):\n",
    "        super(resnet_classifier, self).__init__()\n",
    "    \n",
    "        D_in, H, D_out = 1000, 500, 37\n",
    "        self.resnet50 = pre_model\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image):\n",
    "        outputs = self.resnet50(image)\n",
    "        \n",
    "        fc = self.classifier(outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "047cbbef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "resnet_classifier(\n",
       "  (resnet50): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=500, bias=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): ReLU()\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "    (2): Linear(in_features=500, out_features=37, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vision_module = resnet_classifier(pre_model)\n",
    "vision_module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edba4cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMG_SAVE_PATH = \"C:/Users/ANDlab3/Desktop/multimodal/visionAndNLPmodel/model/\"\n",
    "vision_module.load_state_dict(torch.load(IMG_SAVE_PATH+'vision_module_fine_tuned.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60dba931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# model freeze\n",
    "for i, (name, param) in enumerate(vision_module.named_parameters()):\n",
    "    \n",
    "    param.requires_grad = False\n",
    "    if i == 158:\n",
    "        print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668d386",
   "metadata": {},
   "source": [
    "# NLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ba469c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        @param    bert: a BertModel object\n",
    "        @param    classifier: a torch.nn.Module classifier\n",
    "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
    "        D_in, H, D_out = 768, 500, 37\n",
    "\n",
    "        # Instantiate BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Instantiate an one-layer feed-forward classifier\n",
    "        self.dense = nn.Sequential(\n",
    "                nn.Linear(D_in, H),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "#         # Freeze the BERT model\n",
    "#         if freeze_bert:\n",
    "#             for param in self.bert.parameters():\n",
    "#                 param.requires_grad = False\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Feed input to BERT and the classifier to compute logits.\n",
    "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
    "                      max_length)\n",
    "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
    "                      information with shape (batch_size, max_length)\n",
    "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
    "                      num_labels)\n",
    "        \"\"\"\n",
    "        # Feed input to BERT\n",
    "        outputs = self.bert(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "\n",
    "        # Feed input to classifier to compute logits\n",
    "        \n",
    "        dense = self.dense(last_hidden_state_cls)\n",
    "        logits = self.classifier(dense)\n",
    "\n",
    "        return dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce618bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "NLP_module = BertClassifier()\n",
    "NLP_module= NLP_module.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c38910ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP_SAVE_PATH = \"C:/Users/ANDlab3/Desktop/multimodal/visionAndNLPmodel/model/\"\n",
    "NLP_module.load_state_dict(torch.load(NLP_SAVE_PATH+'nlp_fine_tuned.pt')) # 모델 파라메타 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a5614f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "# model freeze\n",
    "for i, (name, param) in enumerate(NLP_module.named_parameters()):\n",
    "    \n",
    "    param.requires_grad = False\n",
    "    if i == 198:\n",
    "        print('end')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8573ba31",
   "metadata": {},
   "source": [
    "# LanguageAndVisionConcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9d5cfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageAndVisionConcat(nn.Module):    \n",
    "    def __init__(\n",
    "        self,\n",
    "        nlp_module,\n",
    "        vision_module,\n",
    "        num_classes,\n",
    "        language_feature_dim,\n",
    "        vision_feature_dim,\n",
    "        fusion_output_size,\n",
    "        dropout_p):\n",
    "        super(LanguageAndVisionConcat, self).__init__()\n",
    "        self.nlp_module = nlp_module\n",
    "        self.vision_module = vision_module\n",
    "        self.fusion = torch.nn.Linear(\n",
    "                        in_features=(language_feature_dim + vision_feature_dim), \n",
    "                        out_features=fusion_output_size,\n",
    "        )\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features=fusion_output_size, \n",
    "            out_features=num_classes\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, text, text2 , image):\n",
    "        text_features = torch.nn.functional.relu(\n",
    "            self.nlp_module(text, text2)\n",
    "        )\n",
    "        image_features = torch.nn.functional.relu(\n",
    "            self.vision_module(image)\n",
    "        )\n",
    "        combined = torch.cat(\n",
    "            [text_features, image_features], dim=1\n",
    "        )\n",
    "        fused = self.dropout(\n",
    "            torch.nn.functional.relu(\n",
    "            self.fusion(combined)\n",
    "            ))\n",
    "        logits = self.fc(fused)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2290753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageAndVisionConcat(\n",
       "  (nlp_module): BertClassifier(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (dense): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=500, bias=True)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=500, out_features=37, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (vision_module): resnet_classifier(\n",
       "    (resnet50): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=500, bias=True)\n",
       "    )\n",
       "    (classifier): Sequential(\n",
       "      (0): ReLU()\n",
       "      (1): Dropout(p=0.2, inplace=False)\n",
       "      (2): Linear(in_features=500, out_features=37, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fusion): Linear(in_features=1000, out_features=250, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc): Linear(in_features=250, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LanguageAndVisionConcat(\n",
    "        nlp_module = NLP_module,\n",
    "        vision_module = vision_module,\n",
    "        num_classes = class_num,\n",
    "        language_feature_dim = 500,\n",
    "        vision_feature_dim = 500,\n",
    "        fusion_output_size = 250,\n",
    "        dropout_p = 0.5\n",
    ")\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de46ee4",
   "metadata": {},
   "source": [
    "# concat model train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e309a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(),\n",
    "                  lr=5e-5,    # Default learning rate\n",
    "                  eps=1e-8    # Default epsilon value\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bdd19b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train loss: 1.961  val loss: 0.471 Acc: 96.97 time_elapsed: 28 second\n",
      "[2] Train loss: 0.316  val loss: 0.106 Acc: 97.41 time_elapsed: 27 second\n",
      "[3] Train loss: 0.109  val loss: 0.083 Acc: 97.33 time_elapsed: 27 second\n",
      "[4] Train loss: 0.074  val loss: 0.069 Acc: 97.43 time_elapsed: 27 second\n",
      "[5] Train loss: 0.061  val loss: 0.069 Acc: 97.45 time_elapsed: 27 second\n",
      "[6] Train loss: 0.052  val loss: 0.068 Acc: 97.51 time_elapsed: 27 second\n",
      "[7] Train loss: 0.051  val loss: 0.070 Acc: 97.54 time_elapsed: 27 second\n",
      "[8] Train loss: 0.048  val loss: 0.070 Acc: 97.57 time_elapsed: 27 second\n",
      "[9] Train loss: 0.047  val loss: 0.071 Acc: 97.62 time_elapsed: 27 second\n",
      "[10] Train loss: 0.045  val loss: 0.075 Acc: 97.64 time_elapsed: 27 second\n",
      "[11] Train loss: 0.045  val loss: 0.072 Acc: 97.66 time_elapsed: 27 second\n",
      "[12] Train loss: 0.043  val loss: 0.064 Acc: 97.66 time_elapsed: 27 second\n",
      "[13] Train loss: 0.041  val loss: 0.075 Acc: 97.67 time_elapsed: 27 second\n",
      "[14] Train loss: 0.041  val loss: 0.067 Acc: 97.69 time_elapsed: 26 second\n",
      "[15] Train loss: 0.037  val loss: 0.061 Acc: 97.71 time_elapsed: 26 second\n",
      "[16] Train loss: 0.041  val loss: 0.065 Acc: 97.72 time_elapsed: 27 second\n",
      "[17] Train loss: 0.036  val loss: 0.074 Acc: 97.75 time_elapsed: 26 second\n",
      "[18] Train loss: 0.037  val loss: 0.074 Acc: 97.75 time_elapsed: 27 second\n",
      "[19] Train loss: 0.035  val loss: 0.074 Acc: 97.78 time_elapsed: 27 second\n",
      "[20] Train loss: 0.037  val loss: 0.070 Acc: 97.79 time_elapsed: 27 second\n",
      "[21] Train loss: 0.034  val loss: 0.078 Acc: 97.79 time_elapsed: 27 second\n",
      "[22] Train loss: 0.034  val loss: 0.073 Acc: 97.8 time_elapsed: 26 second\n",
      "[23] Train loss: 0.033  val loss: 0.066 Acc: 97.8 time_elapsed: 27 second\n",
      "[24] Train loss: 0.036  val loss: 0.076 Acc: 97.81 time_elapsed: 26 second\n",
      "[25] Train loss: 0.034  val loss: 0.067 Acc: 97.83 time_elapsed: 27 second\n",
      "[26] Train loss: 0.030  val loss: 0.080 Acc: 97.83 time_elapsed: 26 second\n",
      "[27] Train loss: 0.030  val loss: 0.072 Acc: 97.83 time_elapsed: 26 second\n",
      "[28] Train loss: 0.036  val loss: 0.074 Acc: 97.83 time_elapsed: 27 second\n",
      "[29] Train loss: 0.032  val loss: 0.074 Acc: 97.83 time_elapsed: 27 second\n",
      "[30] Train loss: 0.034  val loss: 0.079 Acc: 97.84 time_elapsed: 27 second\n",
      "[31] Train loss: 0.034  val loss: 0.077 Acc: 97.84 time_elapsed: 27 second\n",
      "[32] Train loss: 0.035  val loss: 0.070 Acc: 97.85 time_elapsed: 27 second\n",
      "[33] Train loss: 0.032  val loss: 0.070 Acc: 97.86 time_elapsed: 27 second\n",
      "[34] Train loss: 0.034  val loss: 0.074 Acc: 97.87 time_elapsed: 27 second\n",
      "[35] Train loss: 0.031  val loss: 0.076 Acc: 97.87 time_elapsed: 27 second\n",
      "Accuracy of the network on the val images: 97 %\n",
      "Best epoch: 35 Best Acc: 97.87\n"
     ]
    }
   ],
   "source": [
    "PATH =\"C:/Users/ANDlab3/Desktop/multimodal/visionAndNLPmodel/model/\"\n",
    "\n",
    "train_loss = [] # 그래프를 그리기 위한 loss 저장용 리스트 \n",
    "eval_loss = []\n",
    "correct = 0\n",
    "total = 0\n",
    "best_acc = 0\n",
    "best_epoch = 0\n",
    "\n",
    "epoch = 35\n",
    "# log_interval = 100\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    time_epoch = time.time()\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for data in train_dataloader:\n",
    "        b_input_ids, b_attn_mask, b_labels, v_image, v_label = tuple(t.to(device) for t in data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(b_input_ids, b_attn_mask, v_image)\n",
    "        loss = criterion(outputs, v_label) # 손실함수 계산\n",
    "        loss.backward() # 손실함수 기준으로 역전파 선언\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "         # Update parameters and the learning rate\n",
    "        optimizer.step() # 가중치 최적화 \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    train_loss.append(running_loss / len(train_dataloader))   \n",
    "    \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_running_loss = 0.0\n",
    "        for data in val_dataloader:\n",
    "            b_input_ids, b_attn_mask, b_labels, v_image, v_label = tuple(t.to(device) for t in data)\n",
    "\n",
    "            outputs = model(b_input_ids, b_attn_mask, v_image) \n",
    "            val_loss = criterion(outputs, v_label) \n",
    "            eval_running_loss += val_loss.item()\n",
    "\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "\n",
    "            total += v_label.size(0)\n",
    "\n",
    "            correct += (pred == v_label).sum().item()\n",
    "            acc =  (100 * correct / total)\n",
    "    \n",
    "        eval_loss.append(eval_running_loss / len(val_dataloader)) \n",
    "        \n",
    "        time_elapsed = time.time() - time_epoch\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), PATH+'best_model.pt')\n",
    "            best_epoch = epoch+1\n",
    "        \n",
    "        print('[%d] Train loss: %.3f' %(epoch + 1, running_loss / len(train_dataloader))\n",
    "     ,' val loss: %.3f' %(eval_running_loss / len(val_dataloader))\n",
    "     ,'Acc:', round(acc,2) ,'time_elapsed:', round(time_elapsed),'second')\n",
    "        \n",
    "print('Accuracy of the network on the val images: %d %%' % (100 * correct / total))\n",
    "print('Best epoch:', best_epoch, 'Best Acc:', round(best_acc, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b9fce81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(eval_loss)\n",
    "plt.legend(['train','validation'])\n",
    "plt.title(\"Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00756cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
